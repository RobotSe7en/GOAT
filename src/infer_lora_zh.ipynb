{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9d3bb-1bb0-4d31-aa25-eace4e53a42d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67251c4-9cd2-422c-a2dc-83b9fccdef47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a811b21-7ec3-4f57-b940-76473d1828c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    '../llama-13b/',\n",
    "    load_in_8bit=True,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a6d42e-be65-4f1d-a575-d7643b483c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_model_path = '../models/GOAT_001_13B_Lora/'\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, \n",
    "    lora_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={'':0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31901cd-ed27-4779-94fd-28dbea1efa9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained('../llama-13b/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6b090-0512-40d8-947f-a9de96cdca07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copied from fastchat/train.py\n",
    "def smart_tokenizer_and_embedding_resize(special_tokens_dict, tokenizer, model):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "    如果更改了词表，则重新更改词表和tokenizer的词表尺寸，新添加的词表embedding\n",
    "    用之前词表的embedding均值表示\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "# 2023.04.06 add pad token and resize embedding\n",
    "smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict=dict(pad_token='[PAD]'),\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")\n",
    "# add special tokens\n",
    "add_token = \"</s>\"\n",
    "tokenizer.add_special_tokens({\n",
    "    \"eos_token\": add_token,\n",
    "    \"bos_token\": add_token,\n",
    "    \"unk_token\": add_token,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd85d16-6a9a-46eb-aae0-fb38a048b01e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2023.04.04 用于有监督训练数据的处理\n",
    "def generate_alpaca_prompt(example):\n",
    "    '''\n",
    "    生成中问alpaca类数据集的prompt\n",
    "    '''\n",
    "    if example['input']:\n",
    "        source = f\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to human's question.\\n### Human: {example['instruction']} {example['input']}\\n### Assistant: \"\n",
    "        # source = f'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{example[\"instruction\"]}\\n\\n### Input:\\n{example[\"input\"]}\\n\\n### Response:\\n'\n",
    "        target = f'{example[\"output\"]}'\n",
    "        return dict(example=(source + target, source))\n",
    "    else:\n",
    "        source = f\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to human's question.\\n### Human: {example['instruction']}\\n### Assistant: \"\n",
    "        # source = f'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{example[\"instruction\"]}\\n\\n### Response:\\n'\n",
    "        target = f'{example[\"output\"]}'\n",
    "        return dict(example=(source + target, source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a38fc0-46d0-45fb-b285-0dfec04b62d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = {\n",
    "    \"instruction\": \"编辑以下句子并使其更自然。我想告诉你，我昨天晚上走了一条适合散步的小路。\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"\"\n",
    "}\n",
    "text = generate_alpaca_prompt(text)['example'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89efed0-b4a2-4795-8d5f-753f3e5b1328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18452c38-8016-4bde-8d52-3069ee2e52aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=256,\n",
    "    repetition_penalty=2.0\n",
    ")\n",
    "with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839b3f1-551d-4739-9502-767753e1cf80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = tokenizer.batch_decode(preds)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67af29b-e2d4-4426-8caf-a8408499e316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42408d7-34c7-4955-8c7a-cbd237e1ae88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
