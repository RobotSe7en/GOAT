{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>使用LoRa低资源指令微调Llama(中文)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    '../llama-13b',\n",
    "    device_map='auto',\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained('../llama-13b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copied from fastchat/train.py\n",
    "def smart_tokenizer_and_embedding_resize(special_tokens_dict, tokenizer, model):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "    如果更改了词表，则重新更改词表和tokenizer的词表尺寸，新添加的词表embedding\n",
    "    用之前词表的embedding均值表示\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "# 2023.04.06 add pad token and resize embedding\n",
    "smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict=dict(pad_token='[PAD]'),\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")\n",
    "# add special tokens\n",
    "add_token = \"</s>\"\n",
    "tokenizer.add_special_tokens({\n",
    "    \"eos_token\": add_token,\n",
    "    \"bos_token\": add_token,\n",
    "    \"unk_token\": add_token,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理对话类数据(使用非对话类数据时此节不用执行)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "df = pd.read_json('../datasets/sg_90k_part1_html_cleaned.json')\n",
    "df = df[:10000]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023.04.13 用于对话的有监督训练数据的处理\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Conversation:\n",
    "    '''\n",
    "    多轮对话数据集类\n",
    "    '''\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.prompt = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.</s>\"\n",
    "        self.sep = '</s>'\n",
    "        self.r1 = 'Human: '\n",
    "        self.r2 = 'Assistant: '\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_id = self.get_sep_id()\n",
    "    \n",
    "    \n",
    "    def get_sep_id(self):\n",
    "        '''\n",
    "        获取unmask的开始和结束ids\n",
    "        '''\n",
    "        sep_id = self.tokenizer(self.sep).input_ids[1]\n",
    "        return sep_id\n",
    "        \n",
    "    \n",
    "    def generate_conversation_prompt(self, example):\n",
    "        '''\n",
    "        拼接对话数据集\n",
    "        '''\n",
    "        conversation = f'{self.prompt}'\n",
    "        for idx, content in enumerate(example):\n",
    "            if idx == 0 and content['from'].lower() != 'human':\n",
    "                conversation = ''\n",
    "                break\n",
    "            if content['from'].lower() == 'human':\n",
    "                sentence = self.r1 + content['value'] + self.sep\n",
    "                conversation += sentence\n",
    "            elif content['from'].lower() == 'gpt':\n",
    "                sentence = self.r2 + content['value'] + self.sep\n",
    "                conversation += sentence\n",
    "            else:\n",
    "                conversation = ''\n",
    "                break\n",
    "        return conversation\n",
    "    \n",
    "    \n",
    "    def preprocess(self, examples):\n",
    "        '''\n",
    "        有监督对话数据预处理\n",
    "        TODO: 是否有更优雅的方式处理？\n",
    "        '''\n",
    "        inputs = [\n",
    "            tokenizer(\n",
    "                ex,\n",
    "                return_tensors='pt',\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True\n",
    "                \n",
    "            ) \n",
    "            for ex in examples['conversations']\n",
    "        ]\n",
    "        input_ids = [i.input_ids[0] for i in inputs]\n",
    "        attention_mask = [i.attention_mask[0] for i in inputs]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        sep_idxs = [torch.where(label==self.sep_id)[0].tolist() for label in labels]\n",
    "        for sep_idx, label in zip(sep_idxs, labels):\n",
    "            if len(sep_idx)<3:\n",
    "                continue\n",
    "            label[:sep_idx[1] + 1] = -100\n",
    "            cur_len = sep_idx[1]\n",
    "            count = 3\n",
    "            for idx in sep_idx[2:]:\n",
    "                if count % 2 != 0:\n",
    "                    cur_len = idx\n",
    "                else:\n",
    "                    label[cur_len+1: idx+1] = -100\n",
    "                count += 1\n",
    "        return dict(\n",
    "            input_ids = input_ids,\n",
    "            labels = labels,\n",
    "            attention_mask = attention_mask\n",
    "        )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Conversation(tokenizer, max_length=512)\n",
    "df['conversations'] = df['conversations'].map(conv.generate_conversation_prompt)\n",
    "df['conversations'][98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.train_test_split(train_size=0.9, shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(\n",
    "    conv.preprocess,\n",
    "    batched=True,\n",
    "    batch_size=1000\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data['train']\n",
    "val_data = data['test']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据(使用对话类数据时此节不用执行)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 读取alpaca类数据集\n",
    "df = pd.read_json('../datasets/goat_50k.json')\n",
    "data = Dataset.from_pandas(df)\n",
    "data = data.train_test_split(train_size=0.9, shuffle=True, seed=42)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2023.04.04 用于有监督训练数据的处理\n",
    "def generate_alpaca_prompt(example):\n",
    "    '''\n",
    "    生成中文alpaca类数据集的prompt\n",
    "    '''\n",
    "    if example['input']:\n",
    "        source = f\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n### Human: {example['instruction']}\\n{example['input']}\\n\\n### Assistant: \"\n",
    "        target = f'{example[\"output\"]}'\n",
    "        return dict(example=(source + target, source))\n",
    "    else:\n",
    "        source = f\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\\n\\n### Human: {example['instruction']}\\n\\n### Assistant: \"\n",
    "        target = f'{example[\"output\"]}'\n",
    "        return dict(example=(source + target, source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.map(lambda x: generate_alpaca_prompt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "ignore_index = -100\n",
    "def preprocess(examples):\n",
    "    '''\n",
    "    tokenize inputs和labels，同时mask标签(labels)中的inputs部分\n",
    "    '''\n",
    "    tokenized = [tokenizer(\n",
    "        example,\n",
    "        return_tensors='pt',\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    ) for example in examples['example']]\n",
    "    input_ids = [t.input_ids[0] for t in tokenized]\n",
    "    attention_mask = [t.attention_mask[0] for t in tokenized]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    source_input_ids_lens = [t.input_ids[1].ne(tokenizer.pad_token_id).sum().item() for t in tokenized]\n",
    "    for label, source_len in zip(labels, source_input_ids_lens):\n",
    "        label[:source_len] = -100\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    batch_size=1000\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = data['train']\n",
    "val_data = data['test']\n",
    "# train_data.set_format(type='torch', columns=['input_ids', 'labels', 'attention_mask'])\n",
    "# val_data.set_format(type='torch', columns=['input_ids', 'labels', 'attention_mask'])\n",
    "train_data[0]['example']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.batch_decode([train_data[0]['input_ids']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model, get_peft_model_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainArgs = TrainingArguments(\n",
    "    output_dir= '../ckps',\n",
    "    do_train=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_int8_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=trainArgs,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n",
    ").__get__(model, type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model_state_dict\n",
    "model.save_pretrained('../ckps/GOAT_002')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = {\n",
    "    \"instruction\": \"介绍一下中国的首都\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"\"\n",
    "}\n",
    "text = generate_alpaca_prompt(text)['example'][0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "input_ids = inputs['input_ids'].to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=0.7,\n",
    "        top_k=40,\n",
    "        num_beams=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=256,\n",
    "        generation_config=generation_config,\n",
    "        repetition_penalty=2.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = tokenizer.batch_decode(preds)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
